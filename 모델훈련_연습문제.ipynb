{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hseyeon1006/ESAA_OB/blob/%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C/%EB%AA%A8%EB%8D%B8%ED%9B%88%EB%A0%A8_%EC%97%B0%EC%8A%B5%EB%AC%B8%EC%A0%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **| 모델 훈련 연습 문제**\n",
        "___\n",
        "- 출처 : 핸즈온 머신러닝 Ch04 연습문제 1, 5, 9, 10\n",
        "- 개념 문제의 경우 텍스트 셀을 추가하여 정답을 적어주세요."
      ],
      "metadata": {
        "id": "zCu72vDHGMHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. 수백만 개의 특성을 가진 훈련 세트에서는 어떤 선형 회귀 알고리즘을 사용할 수 있을까요?**\n",
        "___\n"
      ],
      "metadata": {
        "id": "j3g-_Dq9GiuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률적 경사 하강법, 미니배치 경사 하강법"
      ],
      "metadata": {
        "id": "hs4KmTDSz4mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. 배치 경사 하강법을 사용하고 에포크마다 검증 오차를 그래프로 나타내봤습니다. 검증 오차가 일정하게 상승되고 있다면 어떤 일이 일어나고 있는 걸까요? 이 문제를 어떻게 해결할 수 있나요?**\n",
        "___"
      ],
      "metadata": {
        "id": "-pDjW5XcHPOt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습률이 너무 크면 알고리즘이 이리저리 널뛰면서 스텝마다 최적점에서 점점 더 멀어져 발산한다. 이를 해결하기 위해서는 학습률을 낮추어야 한다."
      ],
      "metadata": {
        "id": "WIp-bFVaMqiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. 릿지 회귀를 사용했을 때 훈련 오차가 검증 오차가 거의 비슷하고 둘 다 높았습니다. 이 모델에는 높은 편향이 문제인가요, 아니면 높은 분산이 문제인가요? 규제 하이퍼파라미터 $\\alpha$를 증가시켜야 할까요 아니면 줄여야 할까요?**\n",
        "___"
      ],
      "metadata": {
        "id": "nM7JbsLoy7b7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "높은 편향이 문제 -> 규제 하이퍼파라미터 alpha를 줄여야 함"
      ],
      "metadata": {
        "id": "dxT7G7gCNMho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. 다음과 같이 사용해야 하는 이유는?**\n",
        "___\n",
        "- 평범한 선형 회귀(즉, 아무런 규제가 없는 모델) 대신 릿지 회귀\n",
        "- 릿지 회귀 대신 라쏘 회귀\n",
        "- 라쏘 회귀 대신 엘라스틱넷"
      ],
      "metadata": {
        "id": "C8tARu-ZzOGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 일반적으로 규제가 있는 모델이 규제가 없는 모델보다 성능이 좋음\n",
        "- 라쏘 회귀는 l1 페널티를 사용하여, 가중치를 0으로 만들고, 릿지 회귀는 원점에 가까워지지만 0이 되지는 않는다. 라쏘 회귀는 가장 중요한 가중치를 제외하고는 모두 0이 되는 희소한 모델을 만든다. 또한 자동으로 특성 선택의 효과를 가지므로, 일부 특성만 실제로 유용할 것 같을 때 사용하면 좋다. 확신이 없다면 릿지 회귀를 사용해야 한다.\n",
        "- 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 라쏘보다는 엘라스틱넷을 선호한다."
      ],
      "metadata": {
        "id": "oH6e-J2-0ump"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **추가) 조기 종료를 사용한 배치 경사 하강법으로 소프트맥스 회귀를 구현해보세요(사이킷런은 사용하지 마세요)**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QIZpOEYJVIAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로드 _ iris data\n",
        "X = iris[\"data\"][:, (2, 3)]  # 꽃잎 길이, 꽃잎 넓이\n",
        "y = iris[\"target\"]"
      ],
      "metadata": {
        "id": "-gDmKnYKPXPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모든 샘플에 bias 추가\n",
        "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
      ],
      "metadata": {
        "id": "bB8s0CvqPXNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#랜덤 시드 지정\n",
        "np.random.seed(2042)"
      ],
      "metadata": {
        "id": "Q8MvBIe8PXL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "test_ratio = 0.2\n",
        "validation_ratio = 0.2\n",
        "total_size = len(X_with_bias)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "validation_size = int(total_size * validation_ratio)\n",
        "train_size = total_size - test_size - validation_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X_with_bias[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
        "y_valid = y[rnd_indices[train_size:-test_size]]\n",
        "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "metadata": {
        "id": "-CYpM70_PXKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#클래스 인덱스를 원핫벡터로 바꾸는 간단한 함수 작성\n",
        "def to_one_hot(y):\n",
        "    n_classes = y.max() + 1\n",
        "    m = len(y)\n",
        "    Y_one_hot = np.zeros((m, n_classes))\n",
        "    Y_one_hot[np.arange(m), y] = 1\n",
        "    return Y_one_hot"
      ],
      "metadata": {
        "id": "Hhhm9NV1PXIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:10]"
      ],
      "metadata": {
        "id": "-wFyxQcNPXGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_one_hot(y_train[:10])"
      ],
      "metadata": {
        "id": "3XSn0niuPXEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#훈련 세트와 테스트 세트의 타깃 클래스 확률을 담은 행렬 만들기\n",
        "Y_train_one_hot = to_one_hot(y_train)\n",
        "Y_valid_one_hot = to_one_hot(y_valid)\n",
        "Y_test_one_hot = to_one_hot(y_test)"
      ],
      "metadata": {
        "id": "XFc0Lvf7PXCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#소프트맥스 함수 만듦\n",
        "def softmax(logits):\n",
        "    exps = np.exp(logits)\n",
        "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
        "    return exps / exp_sums"
      ],
      "metadata": {
        "id": "VKeidvzwPW_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#입력과 출력의 개수를 정의\n",
        "n_inputs = X_train.shape[1] # == 3 (특성 2개와 편향)\n",
        "n_outputs = len(np.unique(y_train))   # == 3 (3개의 붓꽃 클래스)"
      ],
      "metadata": {
        "id": "ko4saQj-PzVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#비용함수 구현\n",
        "eta = 0.01\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error)\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "id": "GsQMIs83QEfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Theta"
      ],
      "metadata": {
        "id": "9MUIw_OzQJXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#검증 세트에 대한 예측과 정확도를 확인\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "6NCVgcL1QEZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#l2 규제 추가 _ 손실에 l2 페널티 추가 _ 그래디언트에도 항 추가\n",
        "# 학습률 eta도 증가\n",
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    if iteration % 500 == 0:\n",
        "        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "        loss = xentropy_loss + alpha * l2_loss\n",
        "        print(iteration, loss)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients"
      ],
      "metadata": {
        "id": "oBiF_T0oQVLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 성능 확인\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "uN9xkWB6QcoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#조기 종료 추가 _ 매 반복에서 검증 세트에 대한 손실을 계산해서 오차가 증가하기 시작할 때 멈춤\n",
        "eta = 0.1\n",
        "n_iterations = 5001\n",
        "m = len(X_train)\n",
        "epsilon = 1e-7\n",
        "alpha = 0.1  # 규제 하이퍼파라미터\n",
        "best_loss = np.infty\n",
        "\n",
        "Theta = np.random.randn(n_inputs, n_outputs)\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    logits = X_train.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    error = Y_proba - Y_train_one_hot\n",
        "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
        "    Theta = Theta - eta * gradients\n",
        "\n",
        "    logits = X_valid.dot(Theta)\n",
        "    Y_proba = softmax(logits)\n",
        "    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
        "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
        "    loss = xentropy_loss + alpha * l2_loss\n",
        "    if iteration % 500 == 0:\n",
        "        print(iteration, loss)\n",
        "    if loss < best_loss:\n",
        "        best_loss = loss\n",
        "    else:\n",
        "        print(iteration - 1, best_loss)\n",
        "        print(iteration, loss, \"조기 종료!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "Xq4F3OnsQfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#정확도 확인\n",
        "logits = X_valid.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_valid)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "16E9gpa9QkqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 데이터셋에 대한 모델의 예측 그래프 나타내\n",
        "x0, x1 = np.meshgrid(\n",
        "        np.linspace(0, 8, 500).reshape(-1, 1),\n",
        "        np.linspace(0, 3.5, 200).reshape(-1, 1),\n",
        "    )\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "X_new_with_bias = np.c_[np.ones([len(X_new), 1]), X_new]\n",
        "\n",
        "logits = X_new_with_bias.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "zz1 = Y_proba[:, 1].reshape(x0.shape)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], \"g^\", label=\"Iris virginica\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], \"bs\", label=\"Iris versicolor\")\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], \"yo\", label=\"Iris setosa\")\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "contour = plt.contour(x0, x1, zz1, cmap=plt.cm.brg)\n",
        "plt.clabel(contour, inline=1, fontsize=12)\n",
        "plt.xlabel(\"Petal length\", fontsize=14)\n",
        "plt.ylabel(\"Petal width\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.axis([0, 7, 0, 3.5])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vgHYIRpTQmxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트 세트에 대한 모델의 최종 정확도 측정\n",
        "logits = X_test.dot(Theta)\n",
        "Y_proba = softmax(logits)\n",
        "y_predict = np.argmax(Y_proba, axis=1)\n",
        "\n",
        "accuracy_score = np.mean(y_predict == y_test)\n",
        "accuracy_score"
      ],
      "metadata": {
        "id": "akHyUK8NQqrO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}